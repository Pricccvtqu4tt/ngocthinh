
## Ernie: **E**nhanced **R**epresentation from k**N**owledge **I**nt**E**gration

*Ernie* 通过建模海量数据中的词、实体及实体关系，学习真实世界的语义知识。相较于 *Bert* 学习局部语言共现的语义表示，*Ernie* 直接对语义知识进行建模，增强了模型语义表示能力。

这里我们举个例子：

```Learnt by Bert ：哈 [mask] 滨是 [mask] 龙江的省会，[mask] 际冰 [mask] 文化名城。```

```Learnt by Ernie：[mask] [mask] [mask] 是黑龙江的省会，国际 [mask] [mask] 文化名城。```

在 *Bert* 模型中，我们通过『哈』与『滨』的局部共现，即可判断出『尔』字，模型没有学习与『哈尔滨』相关的任何知识。而 *Ernie* 通过学习词与实体的表达，使模型能够建模出『哈尔滨』与『黑龙江』的关系，学到『哈尔滨』是 『黑龙江』的省会以及『哈尔滨』是个冰雪城市。

此外， *Ernie* 引入了百科、新闻、论坛回帖等多源中文语料进行训练。

我们在多个公开的中文数据集合上进行了效果验证，*Ernie* 模型相较 *Bert*， 取得了更好的效果。
